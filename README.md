# BERT_vs_Transformer-XL

A Comparison of Two NLP Frameworks for General Research Purposes

The goal of Natural Language Processing (NLP) is to train computers to analyze human language. The widest-used versions of NLP are used in spell-check and grammar-check programs, but more advanced versions have been developed into tools used for much more than just identifying context within search queries. NLP is becoming increasingly more useful for researchers to summarize large amounts of data or long-form documents without the need for human supervision. Our project will examine two powerful NLP algorithms, BERT and Transformer-XL in their abilities to extract and summarize data from chosen pieces of literature. We will provide each algorithm with the same dataset and judge the results for each algorithm on its accuracy compared to its execution time.

# Weekly Progress Reports

2/14/20: We've uploaded our abstract to the MassURC website and specified our needs for presentation. We created the GitHub repository for this project.

2/22/20: We updated the abstract to have quotes from the two research papers we're going to reference in our project. Ed has been working on setting up his company's server to run BERT, while Vincent and I have been researching how to use/understand the results from both algorithms.

